{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing whole data \n",
    "# TODO :run once again at the end of project\n",
    "#TODO: rename consts\n",
    "# there can be problem ONLY with filtering data\n",
    "import globs\n",
    "import data_preproccesing\n",
    "\n",
    "print(\"Data preprocessing phase. It will take more then a while\")\n",
    "data_preproccesing.filter_whole_data(globs.general.DATA_PATH, globs.general.DATA_BASE_FILTER_PATH, globs.general.ELEMENTS_TO_SAVE_CSV)\n",
    "data_preproccesing.sort_data(globs.general.DATA_BASE_FILTER_PATH, globs.general.DATA_BASE_FILTER_PATH_SORTED , 'Latitude')\n",
    "data_preproccesing.filter_height(globs.general.DATA_BASE_FILTER_PATH_SORTED, globs.general.DATA_FILTER_HEIGHT_PATH_SORTED, globs.general.ELEMENTS_TO_SAVE_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating measurements per month\n",
      "               Captured Time\n",
      "Captured Time               \n",
      "1981-10-31               135\n",
      "1981-11-30                 0\n",
      "1981-12-31                 0\n",
      "1982-01-31                 0\n",
      "1982-02-28                 0\n",
      "...                      ...\n",
      "2019-11-30           1287139\n",
      "2019-12-31           1364859\n",
      "2020-01-31           1231135\n",
      "2020-02-29           1068996\n",
      "2020-03-31            171913\n",
      "\n",
      "[462 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import globs\n",
    "import data_preproccesing\n",
    "print(\"calculating measurements per month\")\n",
    "data_preproccesing.count_measurements_in_month(globs.general.DATA_BASE_FILTER_PATH_SORTED,globs.general.MEASUREMENTS_IN_MONTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for flight data\n"
     ]
    }
   ],
   "source": [
    "import globs\n",
    "import data_preproccesing\n",
    "print(\"Searching for flight data\")\n",
    "data_preproccesing.find_flights_data(globs.general.DATA_FILTER_HEIGHT_PATH_SORTED,globs.general.DATA_FLIGHTS,h=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making data histograms\n",
      "Reading\n",
      "Making plot\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import globs\n",
    "import data_preproccesing\n",
    "\n",
    "print(\"Making data histograms\")\n",
    "data_preproccesing.data_distribution(globs.general.DATA_BASE_FILTER_PATH_SORTED,globs.plots.VALUE_DISTRIBUTION_BUCKETS_PATH ,globs.plots.VALUE_DISTRIBUTION_PLOT_PATH ,globs.plots.HEIGHT_DISTRIBUTION_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing phase it will be much faster\n",
      "Loading csv file for lat: (35.49) - (35.87) lon: (139.45) - (139.95) \n",
      "Analyzing data: total_squares: 125 without_data: 68, with_data: 57, errors: 0\n",
      "Loading csv file for lat: (41.75) - (42.05) lon: (12.27) - (12.73) \n",
      "Analyzing data: total_squares: 107 without_data: 52, with_data: 55, errors: 0\n",
      "Loading csv file for lat: (49.94) - (50.16) lon: (19.7) - (20.35) \n",
      "Analyzing data: total_squares: 242 without_data: 201, with_data: 41, errors: 0\n",
      "Loading csv file for lat: (51.24) - (51.31) lon: (30.1) - (30.34) \n",
      "Analyzing data: total_squares: 21 without_data: 10, with_data: 11, errors: 0\n"
     ]
    }
   ],
   "source": [
    "# preproceessing whole data for specified city it requires preproccessed whole files\n",
    "#TODO change resolution to 3000\n",
    "import globs\n",
    "import data_processing\n",
    "print(\"Data processing phase it will be much faster\")\n",
    "res = 3000\n",
    "ver=False\n",
    "data_processing.proceed_region(globs.cities.TOKYO, file_to_save=globs.cities.TOKYO_FILE,resolution=res,verbose=ver)\n",
    "data_processing.proceed_region(globs.cities.ROME, file_to_save=globs.cities.ROME_FILE,resolution=res,verbose=ver)\n",
    "data_processing.proceed_region(globs.cities.KRAKOW, file_to_save=globs.cities.KRAKOW_FILE,resolution=res,verbose=ver)\n",
    "data_processing.proceed_region(globs.cities.CZARNOBYL, file_to_save=globs.cities.CZARNOBYL_FILE,resolution=res,verbose=ver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing phase it will be much faster\n",
      "Loading csv file for lat: (37) - (58) lon: (-8) - (32) \n",
      "Analyzing data: total_squares: 68018 without_data: 67975, with_data: 43, errors: 0\n",
      "Loading csv file for lat: (26) - (49) lon: (-126) - (-65) \n",
      "Analyzing data: total_squares: 108335 without_data: 106690, with_data: 1645, errors: 0\n",
      "Loading csv file for lat: (48.5) - (51) lon: (12) - (19) \n",
      "Analyzing data: total_squares: 1589 without_data: 1486, with_data: 103, errors: 0\n",
      "Loading csv file for lat: (30) - (46) lon: (128) - (147) \n",
      "Analyzing data: total_squares: 24179 without_data: 23813, with_data: 366, errors: 0\n",
      "Loading csv file for lat: (6) - (35) lon: (-17) - (47) \n",
      "Analyzing data: total_squares: 149272 without_data: 147742, with_data: 1529, errors: 1\n",
      "Loading csv file for lat: (-33) - (6) lon: (11) - (50) \n",
      "Analyzing data: total_squares: 120660 without_data: 117446, with_data: 3214, errors: 0\n",
      "Loading csv file for lat: (53) - (76) lon: (34) - (178) \n",
      "Analyzing data: total_squares: 269602 without_data: 268800, with_data: 802, errors: 0\n",
      "Loading csv file for lat: (8) - (50) lon: (51) - (146) \n",
      "Analyzing data: total_squares: 315899 without_data: 310693, with_data: 5206, errors: 0\n",
      "Loading csv file for lat: (48.9) - (54.5) lon: (14.9) - (24.2) \n",
      "Analyzing data: total_squares: 4562 without_data: 4509, with_data: 53, errors: 0\n"
     ]
    }
   ],
   "source": [
    "# preproceessing whole data for specified region it requires preproccessed whole files\n",
    "#TODO change resolution to 4000\n",
    "import globs\n",
    "import data_processing\n",
    "print(\"Data processing phase it will be much faster\")\n",
    "res = 10000\n",
    "ver = False\n",
    "data_processing.proceed_region(globs.regions.EUROPE, file_to_save=globs.regions.EUROPE_FILE,resolution=res,verbose=ver)\n",
    "data_processing.proceed_region(globs.regions.USA, file_to_save=globs.regions.USA_FILE,resolution=res,verbose=ver)\n",
    "data_processing.proceed_region(globs.regions.CZECH, file_to_save=globs.regions.CZECH_FILE,resolution=res,verbose=ver)\n",
    "data_processing.proceed_region(globs.regions.JAPAN, file_to_save=globs.regions.JAPAN_FILE,resolution=res,verbose=ver)\n",
    "\n",
    "data_processing.proceed_region(globs.regions.AFRIKA_N, file_to_save=globs.regions.AFRIKA_N_FILE,resolution=res,verbose=ver)\n",
    "data_processing.proceed_region(globs.regions.AFRIKA_S, file_to_save=globs.regions.AFRIKA_S_FILE,resolution=res,verbose=ver)\n",
    "data_processing.proceed_region(globs.regions.ASIA_N, file_to_save=globs.regions.ASIA_N_FILE,resolution=res,verbose=ver)\n",
    "data_processing.proceed_region(globs.regions.ASIA_S, file_to_save=globs.regions.ASIA_S_FILE,resolution=res,verbose=ver)\n",
    "data_processing.proceed_region(globs.regions.POLAND, file_to_save=globs.regions.POLAND_FILE,resolution=res,verbose=ver)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading csv file for lat: (37) - (58) lon: (-8) - (32) \n",
      "Loading csv file for lat: (6) - (35) lon: (-17) - (47) \n",
      "Loading csv file for lat: (-33) - (6) lon: (11) - (50) \n",
      "Loading csv file for lat: (53) - (76) lon: (34) - (178) \n",
      "Loading csv file for lat: (8) - (50) lon: (51) - (146) \n"
     ]
    }
   ],
   "source": [
    "# finding places with a lot of records\n",
    "import globs\n",
    "import data_processing\n",
    "# def find_most_popular_locations(board, resolution=10000, path_to_save=None)\n",
    "data_processing.find_most_popular_locations(globs.regions.EUROPE,10000,globs.regions.EUROPE_MOST_POPULAR)\n",
    "data_processing.find_most_popular_locations(globs.regions.AFRIKA_N,10000,globs.regions.AFRIKA_N_MOST_POPULAR)\n",
    "data_processing.find_most_popular_locations(globs.regions.AFRIKA_S,10000,globs.regions.AFRIKA_S_MOST_POPULAR)\n",
    "data_processing.find_most_popular_locations(globs.regions.ASIA_N,10000,globs.regions.ASIA_N_MOST_POPULAR)\n",
    "data_processing.find_most_popular_locations(globs.regions.ASIA_S,10000,globs.regions.ASIA_S_MOST_POPULAR)\n",
    "data_processing.find_most_popular_locations(globs.regions.USA,10000,globs.regions.USA_MOST_POPULAR)\n",
    "data_processing.find_most_popular_locations(globs.regions.JAPAN,10000,globs.regions.JAPAN_MOST_POPULAR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction to create maps for proccessed data\n",
    "\n",
    "#data need preparation first\n",
    "# BUG: Size of each next map generated is bigger then previous\n",
    "def make_maps(load_file, border, key, verbose=False,path=None,zoom=9):\n",
    "    import globs\n",
    "    import grid\n",
    "    import data_processing\n",
    "    import gmaps\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib as mpl\n",
    "    import os\n",
    "    from ipywidgets.embed import embed_minimal_html\n",
    "    squares_basic_data, squares_with_data = data_processing.load_data_from_file(load_file, verbose=verbose)\n",
    "    gmaps.configure(api_key=key)\n",
    "    lat_c, lon_c = grid.calculate_center_point(border)\n",
    "    \n",
    "    #TODO: make function from this\n",
    "    gradient_google_maps = [(0,0,0,0.0),(0, 255,0,0.8), (255, 255,0,0.8),(255, 0,0,0.8)]\n",
    "    tmp = [(rgba[0]/255,rgba[1]/255,rgba[2]/255,rgba[3]) for rgba in gradient_google_maps]\n",
    "    gradient_matplotlib_bar = []\n",
    "    ticks = 200\n",
    "    for i in range(len(tmp) - 1):\n",
    "        if i == 0:\n",
    "            gradient_matplotlib_bar.append(tmp[0])\n",
    "        current = tmp[i]\n",
    "        following = tmp[i+1]\n",
    "        gradient = []\n",
    "        for j in range(4):\n",
    "            gradient.append((following[j]-current[j])/ticks)\n",
    "        for t in range(1,ticks):\n",
    "            rgba_tmp = []\n",
    "            for j in range(4):\n",
    "                rgba_tmp.append(current[j] + t*gradient[j])\n",
    "            gradient_matplotlib_bar.append(tuple(rgba_tmp))\n",
    "    \n",
    "    locations = []\n",
    "    stds = []\n",
    "    means = []\n",
    "    mins = []\n",
    "    maxs = []\n",
    "    for pol, mean, std, min2, max2 in squares_basic_data:\n",
    "        locations.append(grid.calculate_center_point(pol))\n",
    "        stds.append(std)\n",
    "        means.append(mean)\n",
    "        mins.append(min2)\n",
    "        maxs.append(max2)\n",
    "    print(len(locations))\n",
    "\n",
    "    for fig_type, data in [(\"Min\", mins), (\"Std\", stds), (\"Mean\", means), (\"Max\", maxs)]:\n",
    "        data = [d for d in data]\n",
    "        \n",
    "        \n",
    "        fig = gmaps.figure(center=(lat_c, lon_c), zoom_level=zoom)\n",
    "        #TODO: consider adding max_intensity, different for min/max/mean/std\n",
    "        heat_layer = gmaps.heatmap_layer(locations=locations, weights=data, point_radius=35)\n",
    "        heat_layer.gradient = gradient_google_maps\n",
    "        fig.add_layer(heat_layer)\n",
    "        \n",
    "        bar, ax = plt.subplots(figsize=(1, 6))\n",
    "        bar.subplots_adjust(wspace=0.3, hspace=0.0,right = 0.5)\n",
    "\n",
    "        cmap = mpl.colors.ListedColormap(colors=gradient_matplotlib_bar)\n",
    "        norm = mpl.colors.Normalize(vmin=min(data), vmax=max(data))\n",
    "\n",
    "        bar.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),cax=ax, orientation='vertical', label='Unit')\n",
    "        bar.add_axes(ax)\n",
    "        \n",
    "        # TODO: add saving chart\n",
    "        if path is not None:\n",
    "            # path_to_maps/City_name <- passed as path argument\n",
    "            # path_to_maps/City_name/Map_type/bar_type.png\n",
    "            # path_to_maps/City_name/Map_type/map_type.png\n",
    "            main_dir = path+fig_type + '/'\n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "            if not os.path.exists(main_dir):\n",
    "                os.mkdir(main_dir)\n",
    "            # now i ned to save map and bar\n",
    "            embed_minimal_html(main_dir+fig_type+'_map.html', views=fig)\n",
    "            plt.savefig(main_dir+fig_type+'_bar.png')           \n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "key = \"\"\n",
    "import globs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunatelly because of bug in google (or unexpected behaviour) software we need to run it one be one with restarting kernel\n",
    "make_maps(globs.cities.TOKYO_FILE, globs.cities.TOKYO, key, path=globs.cities.TOKYO_MAP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_maps(globs.cities.ROME_FILE, globs.cities.ROME, key, path=globs.cities.ROME_MAP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_maps(globs.cities.KRAKOW_FILE, globs.cities.KRAKOW, key, path=globs.cities.KRAKOW_MAP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_maps(globs.cities.CZARNOBYL_FILE, globs.cities.CZARNOBYL, key, path=globs.cities.CZARNOBYL_MAP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making maps for all processed data for region\n",
    "make_maps(globs.regions.EUROPE_FILE, globs.regions.EUROPE, key, path=globs.regions.EUROPE_MAP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_maps(globs.regions.USA_FILE, globs.regions.USA, key, path=globs.regions.USA_MAP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_maps(globs.regions.CZECH_FILE, globs.regions.CZECH, key, path=globs.regions.CZECH_MAP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_maps(globs.regions.JAPAN_FILE, globs.regions.JAPAN, key, path=globs.regions.JAPAN_MAP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating corelation height and sea lecel for some places (example)\n",
    "import globs\n",
    "import data_processing\n",
    "\n",
    "data_processing.calculate_correlation_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate covarience matrix for cities \n",
    "import data_processing\n",
    "data_processing.calculate_and_save_covariance_matrix(globs.regions.EUROPE,globs.regions.EUROPE_MAP_DIR)\n",
    "data_processing.calculate_and_save_covariance_matrix(globs.regions.USA,globs.regions.USA_MAP_DIR)\n",
    "data_processing.calculate_and_save_covariance_matrix(globs.regions.CZECH,globs.regions.CZECH_MAP_DIR)\n",
    "data_processing.calculate_and_save_covariance_matrix(globs.regions.JAPAN,path=globs.regions.JAPAN_MAP_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate covarience matrix for regions \n",
    "data_processing.calculate_and_save_covariance_matrix(globs.cities.ROME,globs.cities.ROME_MAP_DIR)\n",
    "data_processing.calculate_and_save_covariance_matrix(globs.cities.TOKYO,globs.cities.TOKYO_MAP_DIR)\n",
    "data_processing.calculate_and_save_covariance_matrix(globs.cities.KRAKOW,globs.cities.KRAKOW_MAP_DIR)\n",
    "data_processing.calculate_and_save_covariance_matrix(globs.cities.CZARNOBYL,path=globs.cities.CZARNOBYL_MAP_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
